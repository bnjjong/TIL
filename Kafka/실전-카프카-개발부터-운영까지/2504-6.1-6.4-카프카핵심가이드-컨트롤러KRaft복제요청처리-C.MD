# 📘 제6장: 카프카 내부 메커니즘

> 이 장은 “카프카를 잘 운용하고 튜닝하기 위해 내부를 이해해야 하는 이유”를 다룹니다.
> 설계 철학보다 **브로커·컨트롤러·복제·요청 처리 흐름** 중심으로 설명합니다.

---

## 🧩 6.1 클러스터 멤버십

### 📍 핵심 개념

* **Zookeeper**는 카프카 클러스터의 브로커 목록을 관리함.
* 각 브로커는 고유한 `broker.id`로 식별되며, 시작 시 `/brokers/ids` 경로에 **Ephemeral 노드**를 등록함.
* 이 노드는 브로커와의 연결이 끊어지면 자동 삭제되어 클러스터가 해당 브로커의 “이탈”을 인식함.

### ⚙️ 동작 흐름

1. 브로커 시작 → Ephemeral 노드 생성 (`/brokers/ids/{broker.id}`)
2. 주키퍼 감시(watch) → 브로커 추가/제거 알림
3. 동일 ID 충돌 시 브로커 시작 실패
4. 브로커 종료/단절 시 → Ephemeral 노드 삭제 → 컨트롤러 감지

> 💡 **즉:** 주키퍼는 “현재 살아있는 브로커의 집합”을 유지하며,
> 이 메커니즘으로 카프카의 동적 리더 선출과 복구가 가능해집니다.

---

## ⚙️ 6.2 컨트롤러 (Controller)

컨트롤러는 **리더 파티션 선출과 메타데이터 관리**의 핵심 역할을 담당합니다.

### 📍 기본 원리

* 클러스터에서 가장 먼저 `/controller` 노드를 등록한 브로커가 컨트롤러가 됨.
* 이 노드 역시 Ephemeral이며, 컨트롤러 종료/단절 시 다른 브로커가 새로 선출됨.
* 주키퍼의 `epoch`(세대 번호)을 사용해 “좀비 컨트롤러”를 방지.

> 🧠 **좀비 컨트롤러:**
> 기존 컨트롤러가 재연결되며 자신이 여전히 유효하다고 착각해 메시지를 전송하는 문제.
> 해결: 각 메시지에 `controllerEpoch` 값을 포함시켜 무효화.

---

### 📦 리더 선출 과정

1. 컨트롤러가 브로커 이탈 감지
2. 이탈한 브로커의 리더 파티션 식별
3. 해당 파티션의 레플리카 목록에서 다음 브로커를 리더로 지정
4. `LeaderAndISR` 요청을 새 리더/팔로워 브로커에 전송
5. `UpdateMetadata` 요청으로 전체 클러스터 메타데이터 갱신

---

### ⚙️ 6.2.1 KRaft: Raft 기반 새로운 컨트롤러

#### 🔹 등장 배경

Zookeeper 기반 컨트롤러의 한계:

* 동기/비동기 처리 불일치로 인한 메타데이터 불일치
* 재시작 시 모든 메타데이터 재로드로 인한 지연
* 관리 복잡성 (Zookeeper와 Kafka 두 시스템 운용 필요)

#### 🔹 KRaft 구조 개요

| 구성 요소                   | 역할                 |
| ----------------------- | ------------------ |
| **Controller quorum**   | Raft 기반 컨트롤러 노드 집합 |
| **Active Controller**   | 리더 역할, 메타데이터 로그 기록 |
| **Follower Controller** | 로그 복제 및 장애 대비      |
| **Broker**              | 실제 데이터 저장 및 서비스 처리 |

* 메타데이터 변경은 “**로그 기반 이벤트 스트림**”으로 관리됨.
* 모든 브로커는 `MetadataFetch` API로 컨트롤러에서 변경사항을 pull함.

#### 🔹 브리지 릴리스 (Bridge Release)

| 단계                 | 설명                          |
| ------------------ | --------------------------- |
| **Pre-KRaft**      | 기존 Zookeeper 기반             |
| **Bridge Release** | KRaft 컨트롤러 + 기존 브로커 혼용      |
| **Post-KRaft**     | 완전한 KRaft 기반 (Zookeeper 제거) |

---

### 🧰 KRaft 설정 요약

| 설정 키                       | 설명                                |
| -------------------------- | --------------------------------- |
| `process.roles`            | controller, broker, 또는 둘 다 지정     |
| `node.id`                  | 고유 노드 ID                          |
| `controller.quorum.voters` | 컨트롤러 쿼럼 노드 정의                     |
| `listeners`                | 역할별 포트 지정 (예: CONTROLLER://:9093) |
| `log.dirs`                 | 브로커/메타데이터 로그 경로                   |

> 💡 KRaft 클러스터 초기화:

```bash
$ bin/kafka-storage.sh random-uuid
$ bin/kafka-storage.sh format -t <CLUSTER_ID> -c config/kraft/controller.properties
```

---

## 🔁 6.3 복제 (Replication)

### 📍 복제 구조

| 유형           | 설명                  |
| ------------ | ------------------- |
| **리더 레플리카**  | 쓰기/읽기 요청을 처리        |
| **팔로워 레플리카** | 리더 데이터를 복제하여 동기화 유지 |

* 리더 장애 시 인싱크 레플리카(ISR) 중 하나가 리더로 승격.
* `replica.lag.time.max.ms` 설정으로 동기화 지연 허용 시간 제어.

---

### 🧠 리더-팔로워 복제 로직

1. 팔로워는 리더에 Fetch 요청을 보냄.
2. 리더는 팔로워가 요청한 오프셋 이후의 메시지 전달.
3. 리더는 각 팔로워의 복제 상태를 추적하여 “ISR(In-Sync Replica)” 집합 유지.
4. ISR이 일정 시간(기본 10초) 뒤처지면 “Out-of-Sync”로 분류되어 리더 후보에서 제외.

---

### ⚡ 선호 리더 (Preferred Leader)

* 토픽 생성 시 최초 리더를 **선호 리더**로 등록.
* `auto.leader.rebalance.enable=true` 설정 시 자동 균등화 수행.

> 💡 부하 분산 목적: 모든 파티션이 브로커에 균등하게 분포되도록 함.

---

## 📡 6.4 요청 처리 (Request Handling)

카프카 브로커는 **프로듀서/컨슈머 요청과 내부 복제 요청**을 처리합니다.

### 📦 요청 구조

| 필드             | 설명           |
| -------------- | ------------ |
| API Key        | 요청 타입        |
| API Version    | 요청 버전        |
| Correlation ID | 요청 추적용 고유 ID |
| Client ID      | 클라이언트 식별자    |

---

### ⚙️ 처리 파이프라인

```
[Client]
   ↓
[Acceptor Thread]
   ↓
[Network Processor Thread]
   ↓
[Request Queue]
   ↓
[IO Thread (Request Handler)]
   ↓
[Response Queue → Client]
```

* 각 포트마다 **Acceptor Thread** 1개
* 요청은 **Network Thread → IO Thread**를 거쳐 처리됨
* 지연 응답(예: Consumer poll 대기)은 **Purgatory Queue**에 보관됨

---

### ✉️ 요청 유형

| 유형                  | 설명                              |
| ------------------- | ------------------------------- |
| **쓰기 요청 (Produce)** | 프로듀서가 보냄, acks 설정에 따라 응답 타이밍 다름 |
| **읽기 요청 (Fetch)**   | 컨슈머/팔로워가 보냄, 하이워터마크까지의 메시지 반환   |
| **메타데이터 요청**        | 브로커 클러스터 메타데이터 요청               |
| **어드민 요청**          | 토픽 생성/삭제, ACL 관리 등              |

---

### 🧩 쓰기 요청 (Produce)

* `acks=0`: 비동기 (즉시 성공 응답)
* `acks=1`: 리더 저장 후 응답
* `acks=all`: 모든 ISR 복제 완료 시 응답

메시지는 OS 캐시에 먼저 기록되며, 실제 지속성은 **복제에 의존**함.

---

### 📖 읽기 요청 (Fetch)

* 컨슈머는 “리더 레플리카”에서만 읽기
* **하이워터마크(High-Water Mark)** 이전의 메시지만 읽기 가능
* `fetch.min.bytes`, `fetch.max.wait.ms`로 응답 조건 제어
* `zero-copy` 전송을 사용하여 메모리 복사 최소화 (고성능의 핵심)

---

### 🧠 읽기 세션 캐시 (Fetch Session Cache)

* 동일 컨슈머의 반복 fetch 요청 시 메타데이터를 캐시하여 오버헤드 감소.
* Kafka 2.3+ 버전에서 도입됨.

---

### 🧩 기타 내부 요청

* `LeaderAndISR`: 새 리더 선출 시 브로커 간 전파
* `OffsetCommit`, `OffsetFetch`: 컨슈머 오프셋 관리용
* `ApiVersions`: 브로커가 지원하는 요청 버전 질의

> 💡 **중요:** 클라이언트보다 브로커를 먼저 업그레이드해야 버전 호환 문제가 발생하지 않습니다.

---

## 🧭 전체 요약

| 섹션        | 주요 주제    | 핵심 포인트                     |
| --------- | -------- | -------------------------- |
| **6.1**   | 클러스터 멤버십 | Zookeeper로 브로커 상태 추적       |
| **6.2**   | 컨트롤러     | 리더 선출 + 메타데이터 관리           |
| **6.2.1** | KRaft    | Zookeeper 없는 Raft 기반 컨트롤러  |
| **6.3**   | 복제       | ISR 유지, 선호 리더 리밸런스         |
| **6.4**   | 요청 처리    | Producer/Consumer 요청 파이프라인 |

---

## 💻 Kotlin 예제 — 리더 장애 감지 및 안전 복구

### 🧩 As-Is (단순 브로커 상태 체크)

```kotlin
fun checkBrokerStatus(brokerList: List<Broker>) {
    brokerList.forEach {
        if (!it.isConnected()) {
            println("Broker ${it.id} is offline!")
        }
    }
}
```

---

### 🚀 To-Be (KRaft 기반 장애 감지 및 리더 재선출)

```kotlin
data class Broker(val id: Int, var status: BrokerStatus)
enum class BrokerStatus { ONLINE, OFFLINE }

class Controller {
    private val metadata = mutableMapOf<String, Int>() // topic → leader brokerId

    fun onBrokerFailure(brokerId: Int) {
        println("Broker $brokerId is down. Reassigning leaders...")
        metadata.filterValues { it == brokerId }.keys.forEach { topic ->
            val newLeader = electNewLeader(topic)
            metadata[topic] = newLeader
            println("New leader for $topic: $newLeader")
        }
    }

    private fun electNewLeader(topic: String): Int {
        // 실제 Kafka에서는 ISR 중 하나를 선택하지만, 여기선 랜덤 예시
        return (1..3).filter { it != metadata[topic] }.random()
    }
}

fun main() {
    val controller = Controller()
    controller.onBrokerFailure(1)
}
```

---

### 🧪 Kotest 예시

```kotlin
class ControllerTest : StringSpec({
    "should reassign leader when broker fails" {
        val controller = Controller()
        controller.onBrokerFailure(1)
    }
})
```

---

## 📚 참고 자료

* [KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum](https://cwiki.apache.org/confluence/display/KAFKA/KIP-500)
* [KIP-595: A Raft Protocol for the Metadata Quorum](https://cwiki.apache.org/confluence/display/KAFKA/KIP-595)
* [Confluent Blog - Kafka Replication Explained](https://www.confluent.io/blog/replication-in-apache-kafka/)